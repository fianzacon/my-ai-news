"""
STEP 7: Partnership database generation from analyzed articles.
"""
import logging
import json
from typing import List, Dict
from dataclasses import dataclass
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI

from .models import LotteContextAnalysis
from .config import PipelineConfig, RateLimiter

logger = logging.getLogger(__name__)


@dataclass
class CompanyInfo:
    """Information about a potential partnership company."""
    name: str
    category: str  # solution, case, technology, regulation
    field: str  # AI ê²€ìƒ‰, ê´‘ê³  í”Œë«í¼, etc.
    recent_achievement: str
    collaboration_point: str
    article_url: str


class PartnershipDatabaseGenerator:
    """Extract company information and generate partnership database."""
    
    def __init__(self):
        self.llm = ChatGoogleGenerativeAI(
            model=PipelineConfig.LLM_MODEL,
            temperature=0.1
        )
        self.rate_limiter = RateLimiter(PipelineConfig.LLM_REQUESTS_PER_MINUTE)
    
    def generate_database(
        self, 
        analyses: List[LotteContextAnalysis]
    ) -> List[CompanyInfo]:
        """
        Extract company information from articles and generate database.
        
        Args:
            analyses: Articles with Lotte context analysis
            
        Returns:
            List of CompanyInfo objects
        """
        logger.info("\n" + "=" * 60)
        logger.info("STEP 7: PARTNERSHIP DATABASE GENERATION")
        logger.info("=" * 60)
        
        # Only process direct relevance articles
        direct_analyses = [a for a in analyses if a.industry_relevance == 'direct']
        logger.info(f"Processing {len(direct_analyses)} direct relevance articles...")
        
        all_companies = []
        
        for i, analysis in enumerate(direct_analyses, 1):
            logger.info(f"\n[{i}/{len(direct_analyses)}] Extracting from: {analysis.article.title[:50]}...")
            
            try:
                self.rate_limiter.wait_if_needed()
                companies = self._extract_companies(analysis)
                
                if companies:
                    all_companies.extend(companies)
                    logger.info(f"   âœ… Extracted {len(companies)} companies")
                else:
                    logger.info(f"   â„¹ï¸  No companies extracted")
                    
            except Exception as e:
                logger.error(f"   âš ï¸  Extraction error: {e}")
        
        # Remove duplicates
        logger.info(f"\nğŸ” Deduplicating companies...")
        unique_companies = self._deduplicate_companies(all_companies)
        
        logger.info(f"âœ… Partnership database complete:")
        logger.info(f"   Total companies: {len(unique_companies)}")
        logger.info(f"   By category:")
        
        by_category = {}
        for company in unique_companies:
            by_category[company.category] = by_category.get(company.category, 0) + 1
        
        for cat, count in sorted(by_category.items()):
            logger.info(f"      {cat}: {count}")
        
        return unique_companies
    
    def _extract_companies(
        self, 
        analysis: LotteContextAnalysis
    ) -> List[CompanyInfo]:
        """Extract company information from a single article."""
        
        # Determine category from impact areas and reasoning
        # Default to technology
        category = 'technology'
        
        # Use simple heuristics based on content
        content_lower = f"{analysis.article.title} {analysis.reasoning}".lower()
        
        if any(word in content_lower for word in ['ê´‘ê³ ', 'ë§ˆì¼€íŒ…', 'ì†”ë£¨ì…˜', 'í”Œë«í¼', 'crm']):
            category = 'solution'
        elif any(word in content_lower for word in ['ë„ì…', 'ì‚¬ë¡€', 'í™œìš©', 'ì ìš©', 'êµ¬í˜„']):
            category = 'case'
        elif 'legal / compliance' in analysis.impact_areas or any(word in content_lower for word in ['ê·œì œ', 'ë²•ë¥ ', 'ë²•ì•ˆ', 'ì»´í”Œë¼ì´ì–¸ìŠ¤']):
            category = 'regulation'
        
        prompt = f"""You are extracting company information from an AI news article for a partnership database.

**Article:**
Title: {analysis.article.title}
Content: {analysis.article.full_content[:2000]}...

**Lotte Members Context:**
Impact: {analysis.impact_type}
Reasoning: {analysis.reasoning}

**Task:**
Extract ALL companies/organizations mentioned in this article that could be potential partners.

For EACH company, provide:
1. **name**: Company or organization name (Korean preferred)
2. **field**: Specific AI field/technology (e.g., "AI ê²€ìƒ‰", "ê´‘ê³  í”Œë«í¼", "ê³ ê° ë¶„ì„")
3. **recent_achievement**: What they achieved/announced in THIS article (1 sentence)
4. **collaboration_point**: How Lotte Members could collaborate with them (1 sentence, specific)

**Output Format (JSON array):**
[
  {{
    "name": "ë„¤ì´ë²„",
    "field": "AI ê²€ìƒ‰, ê°œì¸í™” ì¶”ì²œ",
    "recent_achievement": "GPT-4 ê¸°ë°˜ í•˜ì´í¼í´ë¡œë°”X ì¶œì‹œ, ê²€ìƒ‰ ì •í™•ë„ 40% í–¥ìƒ",
    "collaboration_point": "ë¡¯ë°ë©¤ë²„ìŠ¤ êµ¬ë§¤ ë°ì´í„°ë¡œ ê°œì¸í™” ê²€ìƒ‰ ì—”ì§„ êµ¬ì¶• ê°€ëŠ¥"
  }},
  ...
]

**Important:**
- Extract ONLY companies that are actively doing something in AI
- Skip generic mentions ("êµ­ë‚´ ê¸°ì—…ë“¤", "ì—…ê³„" etc.)
- Be specific about field and achievements
- Collaboration point must be actionable for Lotte Members

Respond ONLY with valid JSON array, no additional text."""
        
        try:
            response = self.llm.invoke(prompt).content
            parsed = self._parse_companies_response(response)
            
            companies = []
            for item in parsed:
                companies.append(CompanyInfo(
                    name=item['name'],
                    category=category,
                    field=item['field'],
                    recent_achievement=item['recent_achievement'],
                    collaboration_point=item['collaboration_point'],
                    article_url=analysis.article.url
                ))
            
            return companies
            
        except Exception as e:
            logger.error(f"Company extraction error: {e}")
            return []
    
    def _parse_companies_response(self, response: str) -> List[Dict]:
        """Parse LLM JSON array response."""
        try:
            # Extract JSON array
            start = response.find('[')
            end = response.rfind(']') + 1
            
            if start != -1 and end > start:
                json_str = response[start:end]
                parsed = json.loads(json_str)
            else:
                parsed = json.loads(response)
            
            # Validate each item
            validated = []
            for item in parsed:
                if all(key in item for key in ['name', 'field', 'recent_achievement', 'collaboration_point']):
                    validated.append(item)
            
            return validated
            
        except Exception as e:
            logger.error(f"Error parsing companies response: {e}")
            return []
    
    def _deduplicate_companies(
        self, 
        companies: List[CompanyInfo]
    ) -> List[CompanyInfo]:
        """Remove duplicate companies, keeping the most informative entry."""
        
        # Group by company name (case-insensitive)
        by_name = {}
        for company in companies:
            name_key = company.name.lower().strip()
            
            if name_key not in by_name:
                by_name[name_key] = []
            
            by_name[name_key].append(company)
        
        # Keep one per company (prefer longer achievement descriptions)
        unique = []
        for name_key, company_list in by_name.items():
            # Sort by achievement length (more detailed = better)
            best = max(company_list, key=lambda c: len(c.recent_achievement))
            unique.append(best)
        
        return unique
    
    def save_to_markdown(
        self, 
        companies: List[CompanyInfo], 
        filename: str = "collaboration_partners.md"
    ):
        """
        Save partnership database to Markdown file with tables grouped by field.
        
        Args:
            companies: List of CompanyInfo objects
            filename: Output filename
        """
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y-%m-%d")
        
        # Group by field (AI ê´‘ê³ , ê°œì¸í™” ì¶”ì²œ, etc.)
        by_field = {}
        
        for company in companies:
            # Normalize field names (remove extra spaces, commas)
            fields = [f.strip() for f in company.field.split(',')]
            
            # Use primary field (first one)
            primary_field = fields[0] if fields else 'AI ê¸°ìˆ '
            
            if primary_field not in by_field:
                by_field[primary_field] = []
            
            by_field[primary_field].append(company)
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                # Header
                f.write("# AI í˜‘ì—… ê°€ëŠ¥ ì—…ì²´ ë¦¬ìŠ¤íŠ¸\n\n")
                f.write(f"**ì—…ë°ì´íŠ¸**: {timestamp}\n")
                f.write(f"**ì´ ì—…ì²´ ìˆ˜**: {len(companies)}ê°œ\n\n")
                f.write("---\n\n")
                
                # Field-based tables (grouped by actual business fields)
                field_emoji = {
                    'AI ê´‘ê³ ': 'ğŸ¯',
                    'ê°œì¸í™” ì¶”ì²œ': 'ğŸ”',
                    'AI ë§ˆì¼€íŒ…': 'ğŸ“¢',
                    'ë°ì´í„° ë¶„ì„': 'ğŸ“Š',
                    'ê³ ê° ì¸ì‚¬ì´íŠ¸': 'ğŸ’¡',
                    'ì±—ë´‡': 'ğŸ¤–',
                    'LLM': 'ğŸ§ ',
                    'ê²€ìƒ‰': 'ğŸ”',
                    'ìŒì„±ì¸ì‹': 'ğŸ¤',
                    'ì´ë¯¸ì§€ ìƒì„±': 'ğŸ¨'
                }
                
                # Sort fields by number of companies (descending)
                sorted_fields = sorted(by_field.items(), key=lambda x: len(x[1]), reverse=True)
                
                for field_name, companies_in_field in sorted_fields:
                    emoji = field_emoji.get(field_name, 'ğŸ’¼')
                    
                    f.write(f"## {emoji} {field_name} ({len(companies_in_field)}ê°œ ì—…ì²´)\n\n")
                    
                    # Table
                    f.write("| íšŒì‚¬ëª… | ìµœê·¼ ì„±ê³¼ | í˜‘ì—… í¬ì¸íŠ¸ | ê¸°ì‚¬ ì¶œì²˜ |\n")
                    f.write("|--------|-----------|-------------|----------|\n")
                    
                    for company in companies_in_field:
                        # Truncate long text for table readability
                        achievement = company.recent_achievement[:100] + "..." if len(company.recent_achievement) > 100 else company.recent_achievement
                        collab = company.collaboration_point[:100] + "..." if len(company.collaboration_point) > 100 else company.collaboration_point
                        
                        # Escape pipe characters in content
                        name = company.name.replace('|', '\\|')
                        achievement = achievement.replace('|', '\\|')
                        collab = collab.replace('|', '\\|')
                        
                        f.write(f"| {name} | {achievement} | {collab} | [ë§í¬]({company.article_url}) |\n")
                    
                    f.write("\n---\n\n")
                
                # Footer
                f.write("## ğŸ“Œ í™œìš© ê°€ì´ë“œ\n\n")
                f.write("ë¶„ì•¼ë³„ë¡œ ë¡¯ë°ë©¤ë²„ìŠ¤ì™€ í˜‘ì—… ê°€ëŠ¥í•œ AI ê¸°ì—…ë“¤ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.\n\n")
                f.write("- **ìµœê·¼ ì„±ê³¼**: í•´ë‹¹ ê¸°ì—…ì˜ ìµœì‹  AI í™œìš© ì‚¬ë¡€ ë° ê¸°ìˆ  ì„±ê³¼\n")
                f.write("- **í˜‘ì—… í¬ì¸íŠ¸**: ë¡¯ë°ë©¤ë²„ìŠ¤ì™€ì˜ êµ¬ì²´ì ì¸ í˜‘ì—… ê°€ëŠ¥ì„± ë° ì‹œë„ˆì§€\n\n")
                f.write("---\n\n")
                f.write(f"*Generated by AI News Intelligence Pipeline - {timestamp}*\n")
            
            logger.info(f"\nğŸ’¾ Partnership database saved: {filename}")
            
        except Exception as e:
            logger.error(f"Failed to save partnership database: {e}")
