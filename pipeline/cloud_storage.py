"""
Cloud Storage integration for archiving pipeline results.
"""
import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any
from google.cloud import storage

from .models import LotteContextAnalysis, WebexMessage

logger = logging.getLogger(__name__)

class CloudStorageArchive:
    """Save pipeline results to Google Cloud Storage."""
    
    def __init__(self, bucket_name: str = "lotte-ai-news-archive"):
        self.bucket_name = bucket_name
        self.client = None
        self.bucket = None
        
        try:
            self.client = storage.Client()
            self.bucket = self.client.bucket(bucket_name)
            logger.info(f"âœ… Cloud Storage connected: {bucket_name}")
        except Exception as e:
            logger.warning(f"âš ï¸  Cloud Storage initialization failed: {e}")
            logger.warning("   Results will not be archived to GCS")
    
    def save_daily_results(
        self,
        articles: List[LotteContextAnalysis],
        messages: List[WebexMessage],
        stats: dict = None
    ) -> bool:
        """
        [Stage 1: ìì • ì‹¤í–‰ìš©] ì „ì²´ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ GCSì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        if not self.client or not self.bucket:
            logger.debug("Cloud Storage not configured, skipping archive")
            return False
        
        try:
            date_str = datetime.now().strftime('%Y%m%d')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            result_data = {
                'date': date_str,
                'timestamp': timestamp,
                'analyzed_articles': [],
                'webex_messages': [],
                'stats': stats or {}
            }
            
            # Articles ë°ì´í„° ë³€í™˜ (ì•ˆì „í•˜ê²Œ ì¶”ì¶œ)
            for context in articles:
                # 1. ì›ë³¸ ê¸°ì‚¬ ê°ì²´ ì°¾ê¸° (article, original_article, ë˜ëŠ” context ìì²´)
                article_obj = getattr(context, 'article', None) or getattr(context, 'original_article', None) or context
                
                # 2. ë°ì´í„° ì¶”ì¶œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
                article_data = {
                    'title': getattr(article_obj, 'title', 'No Title'),
                    'url': getattr(article_obj, 'url', ''),
                    'published_date': article_obj.published_date.isoformat() if hasattr(article_obj, 'published_date') and article_obj.published_date else None,
                    'source': getattr(article_obj, 'source', 'Unknown'),
                    'full_content': getattr(article_obj, 'content', '')[:1000] if hasattr(article_obj, 'content') else '',
                    'lotte_context': {
                        'impact_type': getattr(context, 'impact_type', ''),
                        'impact_areas': getattr(context, 'impact_areas', []),
                        'reasoning': getattr(context, 'reasoning', ''),
                        'industry_relevance': getattr(context, 'industry_relevance', ''),
                        'industry_category': getattr(context, 'industry_category', '')
                    }
                }
                result_data['analyzed_articles'].append(article_data)
            
            # Messages ë°ì´í„° ë³€í™˜
            for msg in messages:
                result_data['webex_messages'].append({
                    'text': msg.text,
                    'priority': getattr(msg, 'priority', 'INFO')
                })
            
            # GCS ì €ì¥
            blob_path = f"daily_results/{date_str}/results_{timestamp}.json"
            blob = self.bucket.blob(blob_path)
            blob.upload_from_string(
                json.dumps(result_data, ensure_ascii=False, indent=2),
                content_type='application/json'
            )
            
            logger.info(f"âœ… Saved to GCS: gs://{self.bucket_name}/{blob_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to save to GCS: {e}", exc_info=True)
            return False

    def load_daily_results(self, date_str: Optional[str] = None) -> Optional[Dict]:
        """
        [Stage 2: ì˜¤ì „ 9ì‹œ ì‹¤í–‰ìš©] íŠ¹ì • ë‚ ì§œì˜ ê°€ì¥ ìµœì‹  ê²°ê³¼ë¥¼ GCSì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        if not self.client or not self.bucket:
            logger.error("Cloud Storage not connected")
            return None

        try:
            if date_str is None:
                date_str = datetime.now().strftime('%Y%m%d')
            
            prefix = f"daily_results/{date_str}/"
            logger.info(f"ğŸ” Searching GCS: gs://{self.bucket_name}/{prefix}")
            
            blobs = list(self.client.list_blobs(self.bucket_name, prefix=prefix))
            json_blobs = [b for b in blobs if b.name.endswith('.json')]
            
            if not json_blobs:
                logger.warning(f"âš ï¸  No result files found for {date_str}")
                return None
            
            # ìµœì‹  íŒŒì¼ ì„ íƒ
            latest_blob = sorted(json_blobs, key=lambda x: x.updated, reverse=True)[0]
            logger.info(f"ğŸ“„ Loading: {latest_blob.name}")
            
            content = latest_blob.download_as_text()
            result_data = json.loads(content)
            
            count = len(result_data.get('webex_messages', []))
            logger.info(f"âœ… Loaded {count} messages from GCS")
            return result_data
            
        except Exception as e:
            logger.error(f"âŒ Failed to load from GCS: {e}", exc_info=True)
            return None